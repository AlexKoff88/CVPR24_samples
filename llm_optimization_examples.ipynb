{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM optimization examples with NNCF and OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure that you have enought disk space as the environment and the model can take sevaral gigabytes of you space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for a new environment\n",
    "\n",
    "#%pip install diffusers\n",
    "#%pip install openvino nncf\n",
    "#%pip install git+https://github.com/huggingface/optimum.git\n",
    "#%pip install git+https://github.com/huggingface/optimum-intel.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating-point model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e0f9ab6099402bb5e2abfe8552f819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "WARNING:root:Cannot apply model.to_bettertransformer because of the exception:\n",
      "The model type phi3 is not yet supported to be used with BetterTransformer. Feel free to open an issue at https://github.com/huggingface/optimum/issues if you would like this model type to be supported. Currently supported models are: dict_keys(['albert', 'bark', 'bart', 'bert', 'bert-generation', 'blenderbot', 'bloom', 'camembert', 'blip-2', 'clip', 'codegen', 'data2vec-text', 'deit', 'distilbert', 'electra', 'ernie', 'fsmt', 'gpt2', 'gptj', 'gpt_neo', 'gpt_neox', 'hubert', 'layoutlm', 'm2m_100', 'marian', 'markuplm', 'mbart', 'opt', 'pegasus', 'rembert', 'prophetnet', 'roberta', 'roc_bert', 'roformer', 'splinter', 'tapas', 't5', 'vilt', 'vit', 'vit_mae', 'vit_msn', 'wav2vec2', 'xlm-roberta', 'yolos']).. Usage model with stateful=True may be non-effective if model does not contain torch.functional.scaled_dot_product_attention\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/home/alex/virt_envs/test3.11/lib/python3.11/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/alex/virt_envs/test3.11/lib/python3.11/site-packages/optimum/exporters/onnx/model_patcher.py:300: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/alex/virt_envs/test3.11/lib/python3.11/site-packages/optimum/exporters/openvino/model_patcher.py:1201: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "Compiling the model to CPU ...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage latency per token:  0.46480048232608373\n",
      "Answer:  Hey, model! Tell me about LLM? An LLM, or a Large Language Model, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are based on deep learning algorithms, particularly a variant of the transformer architecture, which allows them to process and analyze vast amounts of text data.\n",
      "\n",
      "Large Language Models are trained on diverse and extensive text corpora, such as books, articles, and websites, enabling them to learn patterns, grammar, and context in human language. Some popular examples of LLMs include GPT-3 (developed by Microsoft), BERT (developed by Google), and T5 (also developed by Google).\n",
      "\n",
      "These models can perform a wide range of language-related tasks, such as:\n",
      "\n",
      "1. Text generation: Creating coherent and contextually relevant text based on a given prompt or topic.\n",
      "2. Text completion: Filling in missing words or phrases in a sentence or paragraph.\n",
      "3. Text summarization: Condensing long pieces of text into shorter, more concise summaries.\n",
      "4. Question answering: Providing accurate and relevant answers to questions based on the information in a given text.\n",
      "5. Sentiment analysis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Phi-3-mini-4k-instruct/tokenizer_config.json',\n",
       " 'Phi-3-mini-4k-instruct/special_tokens_map.json',\n",
       " 'Phi-3-mini-4k-instruct/tokenizer.model',\n",
       " 'Phi-3-mini-4k-instruct/added_tokens.json',\n",
       " 'Phi-3-mini-4k-instruct/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import time\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "LOCAL_PATH = \"Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(MODEL_ID, export=True, load_in_8bit=False, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "prompt = template.format(question)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Avarage latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)\n",
    "\n",
    "model.save_pretrained(LOCAL_PATH)\n",
    "tokenizer.save_pretrained(LOCAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit weight quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading Phi-3-mini-4k-instruct/ requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m MODEL_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3-mini-4k-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mOVModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOCAL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(LOCAL_PATH)\n\u001b[1;32m     11\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|user|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<|end|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<|assistant|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/virt_envs/test3.11/lib/python3.11/site-packages/optimum/modeling_base.py:360\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_id, subfolder)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name \u001b[38;5;241m==\u001b[39m CONFIG_NAME:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CONFIG_NAME \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_id, subfolder)):\n\u001b[0;32m--> 360\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m CONFIG_NAME \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(model_id):\n\u001b[1;32m    364\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    365\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_id, CONFIG_NAME), trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code\n\u001b[1;32m    366\u001b[0m         )\n",
      "File \u001b[0;32m~/virt_envs/test3.11/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1141\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1140\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[0;32m-> 1141\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m   1146\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/virt_envs/test3.11/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:622\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[0;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[1;32m    619\u001b[0m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[0;32m--> 622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires you to execute the configuration file in that\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m     )\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[0;31mValueError\u001b[0m: Loading Phi-3-mini-4k-instruct/ requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import time\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(LOCAL_PATH, quantization_config=dict(bits=8), compile=False, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "prompt = template.format(question)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Average latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-bit data-aware weight quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1195302 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e07565f81d4ba582e968224755e6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "QUANTIZED_PATH = \"Phi-3-mini-4k-instruct-quantized\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(\n",
    "    LOCAL_PATH,\n",
    "    compile=False,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=dict(bits=4, sym=True, ratio=0.8, dataset=\"ptb\", awq=True, scale_estimation=True)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "prompt = template.format(question)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Average latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)\n",
    "\n",
    "model.save_pretrained(QUANTIZED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Quantization and KV-cache Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency per token:  0.053900335889413536\n",
      "Answer:  Hey, model! Tell me about LLM? Large Language Models (LLMs) are a type of artificial intelligence designed to understand, generate, and sometimes translate human language. They are based on deep learning algorithms, particularly neural networks, that have been trained on vast amounts of text data.\n",
      "\n",
      "\n",
      "LLMs like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) have significantly impacted natural language processing (NLP). They can perform a variety of tasks such as text classification, question answering, and language translation.\n",
      "\n",
      "\n",
      "These models work by predicting the likelihood of a sequence of words, which allows them to generate coherent and contextually relevant text. The \"transformer\" architecture, which LLMs often use, is particularly effective because it processes words in relation to all other words in a sentence, rather than one at a time, enabling a deeper understanding of context and nuance.\n",
      "\n",
      "\n",
      "LLMs are continually evolving, with researchers working on improving their capabilities, reducing biases, and making them more efficient and accessible for various applications.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(\n",
    "    QUANTIZED_PATH,\n",
    "    trust_remote_code=True,\n",
    "    ov_config={\"KV_CACHE_PRECISION\": \"u8\", \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\", \"PERFORMANCE_HINT\": \"LATENCY\"},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Average latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
