{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM optimization examples with NNCF and OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure that you have enought disk space as the environment and the model can take sevaral gigabytes of you space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for a new environment\n",
    "\n",
    "#%pip install diffusers\n",
    "#%pip install openvino nncf\n",
    "#%pip install git+https://github.com/huggingface/optimum.git\n",
    "#%pip install git+https://github.com/huggingface/optimum-intel.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating-point model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8399153aabb44c98bfbe1dfe34027fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "WARNING:root:Cannot apply model.to_bettertransformer because of the exception:\n",
      "The model type phi3 is not yet supported to be used with BetterTransformer. Feel free to open an issue at https://github.com/huggingface/optimum/issues if you would like this model type to be supported. Currently supported models are: dict_keys(['albert', 'bark', 'bart', 'bert', 'bert-generation', 'blenderbot', 'bloom', 'camembert', 'blip-2', 'clip', 'codegen', 'data2vec-text', 'deit', 'distilbert', 'electra', 'ernie', 'fsmt', 'gpt2', 'gptj', 'gpt_neo', 'gpt_neox', 'hubert', 'layoutlm', 'm2m_100', 'marian', 'markuplm', 'mbart', 'opt', 'pegasus', 'rembert', 'prophetnet', 'roberta', 'roc_bert', 'roformer', 'splinter', 'tapas', 't5', 'vilt', 'vit', 'vit_mae', 'vit_msn', 'wav2vec2', 'xlm-roberta', 'yolos']).. Usage model with stateful=True may be non-effective if model does not contain torch.functional.scaled_dot_product_attention\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/home/alex/virt_envs/test3.11/lib/python3.11/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/alex/virt_envs/test3.11/lib/python3.11/site-packages/optimum/exporters/onnx/model_patcher.py:300: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/alex/virt_envs/test3.11/lib/python3.11/site-packages/optimum/exporters/openvino/model_patcher.py:1201: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "Compiling the model to CPU ...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage latency per token:  0.23498668140835233\n",
      "Answer:  Hey, model! Tell me about LLM? An LLM, or a Large Language Model, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are based on deep learning algorithms, particularly a variant of the transformer architecture, which allows them to process and analyze vast amounts of text data.\n",
      "\n",
      "Large Language Models are trained on diverse and extensive text corpora, such as books, articles, and websites, enabling them to learn patterns, grammar, and context in human language. Some popular examples of LLMs include GPT-3 (developed by Microsoft), BERT (developed by Google), and T5 (also developed by Google).\n",
      "\n",
      "These models can perform a wide range of language-related tasks, such as:\n",
      "\n",
      "1. Text generation: Creating coherent and contextually relevant text based on a given prompt or topic.\n",
      "2. Text completion: Filling in missing words or phrases in a sentence or paragraph.\n",
      "3. Text summarization: Condensing long pieces of text into shorter, more concise summaries.\n",
      "4. Question answering: Providing accurate and relevant answers to questions based on the information in a given text.\n",
      "5. Sentiment analysis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Phi-3-mini-4k-instruct/tokenizer_config.json',\n",
       " 'Phi-3-mini-4k-instruct/special_tokens_map.json',\n",
       " 'Phi-3-mini-4k-instruct/tokenizer.model',\n",
       " 'Phi-3-mini-4k-instruct/added_tokens.json',\n",
       " 'Phi-3-mini-4k-instruct/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import time\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "LOCAL_PATH = \"Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(MODEL_ID, export=True, load_in_8bit=False, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "prompt = template.format(question)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Avarage latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)\n",
    "\n",
    "model.save_pretrained(LOCAL_PATH)\n",
    "tokenizer.save_pretrained(LOCAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit weight quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (130 / 130)            │ 100% (130 / 130)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7cf6268dd6467a9e554cd9c4516682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Compiling the model to CPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency per token:  0.08655611055868644\n",
      "Answer:  Hey, model! Tell me about LLM? An LLM, or a Large Language Model, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are based on deep learning algorithms, particularly a variant called Transformer architecture, which allows them to process and analyze vast amounts of text data.\n",
      "\n",
      "Large Language Models are trained on diverse and extensive text corpora, such as books, articles, and websites, enabling them to learn patterns, grammar, and contextual information. Some popular examples of LLMs include GPT-3 (developed by Microsoft), BERT (developed by Google), and T5 (Text-to-Text Transfer Transformer).\n",
      "\n",
      "These models can perform a wide range of language-related tasks, such as:\n",
      "\n",
      "1. Text generation: Creating coherent and contextually relevant text based on a given prompt or topic.\n",
      "2. Text completion: Filling in missing words or phrases in a sentence or paragraph.\n",
      "3. Text summarization: Condensing long pieces of text into shorter, more concise summaries.\n",
      "4. Question answering: Providing accurate and relevant answers to questions based on the information available in the training data.\n",
      "5. Sent\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import time\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(LOCAL_PATH, quantization_config=dict(bits=8), compile=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "prompt = template.format(question)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Average latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-bit data-aware weight quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1195302 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b87d90c4734cc39ecc40d35b101d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "QUANTIZED_PATH = \"Phi-3-mini-4k-instruct-quantized\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(\n",
    "    LOCAL_PATH,\n",
    "    compile=False,\n",
    "    quantization_config=dict(bits=4, sym=True, ratio=0.8, dataset=\"ptb\")\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "prompt = template.format(question)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Average latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)\n",
    "\n",
    "model.save_pretrained(QUANTIZED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Quantization and KV-cache Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(\n",
    "    QUANTIZED_PATH,\n",
    "    ov_config={\"KV_CACHE_PRECISION\": \"u8\", \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\", \"PERFORMANCE_HINT\": \"LATENCY\"},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n",
    "\n",
    "template = \"<|user|>\\n{}<|end|>\\n<|assistant|>\"\n",
    "question = \"Hey, model! Tell me about LLM?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(\"Average latency per token: \", (time.time() - start) / output.shape[1])\n",
    "\n",
    "answer = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
